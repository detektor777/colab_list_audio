{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMv7BSno0BRmi1IZw4sOYEp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/detektor777/colab_list_audio/blob/main/whisper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3PN2bsdXI5h"
      },
      "outputs": [],
      "source": [
        "#@title ##**Install** { display-mode: \"form\" }\n",
        "%%capture\n",
        "\n",
        "!pip install -q git+https://github.com/openai/whisper.git\n",
        "!pip install -q git+https://github.com/yt-dlp/yt-dlp.git\n",
        "!pip install -q ipywidgets\n",
        "\n",
        "!pip install -q torch torchaudio\n",
        "!pip install -q moviepy\n",
        "\n",
        "import IPython\n",
        "IPython.display.clear_output()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##**Google Drive** { display-mode: \"form\" }\n",
        "%%capture\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "JB2Hjo5zXgGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##**Run** { display-mode: \"form\" }\n",
        "selected_model = \"large\"         #@param [\"tiny\", \"base\", \"small\", \"medium\", \"large\"]\n",
        "selected_language = \"en\"        #@param [\"ru\", \"en\", \"de\", \"fr\", \"es\", \"it\", \"uk\", \"zh\", \"ja\"]\n",
        "selected_format = \"srt\"         #@param [\"txt\", \"srt\"]\n",
        "show_text = True                #@param {type:\"boolean\"}\n",
        "audio_file_path = \"/content/drive/MyDrive/v.wav\"  #@param {type:\"string\"}\n",
        "\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"  # Предотвращаем фрагментацию памяти\n",
        "\n",
        "import whisper\n",
        "import torch\n",
        "import gc\n",
        "import datetime\n",
        "from IPython.display import display, Audio\n",
        "import ipywidgets as widgets\n",
        "\n",
        "def seconds_to_srt_time(sec):\n",
        "    hours = int(sec // 3600)\n",
        "    minutes = int((sec % 3600) // 60)\n",
        "    seconds = int(sec % 60)\n",
        "    milliseconds = int((sec - int(sec)) * 1000)\n",
        "    return f\"{hours:02d}:{minutes:02d}:{seconds:02d},{milliseconds:03d}\"\n",
        "\n",
        "def extract_audio_segment(audio_path, start_time, end_time):\n",
        "    from pydub import AudioSegment\n",
        "    print(f\"Extracting segment from {start_time} to {end_time}\")\n",
        "    audio = AudioSegment.from_file(audio_path)\n",
        "    start_ms = start_time * 1000\n",
        "    end_ms = end_time * 1000\n",
        "    segment = audio[start_ms:end_ms]\n",
        "    print(\"Segment extracted\")\n",
        "    del audio\n",
        "    return segment\n",
        "\n",
        "def retranscribe_segment(audio_path, start, end, whisper_model, lang, progress_label):\n",
        "    print(\"Preparing for retranscribe...\")\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    try:\n",
        "        progress_label.value = \"Extracting audio...\"\n",
        "        print(\"Calling extract_audio_segment...\")\n",
        "        audio_segment = extract_audio_segment(audio_path, start, end)\n",
        "        temp_file = \"temp_segment.wav\"\n",
        "        print(\"Exporting audio segment to temp file...\")\n",
        "        audio_segment.export(temp_file, format=\"wav\")\n",
        "        del audio_segment\n",
        "        progress_label.value = \"Transcribing...\"\n",
        "        print(\"Starting Whisper transcription...\")\n",
        "        result = whisper_model.transcribe(temp_file, language=lang)\n",
        "        print(\"Transcription completed, cleaning up...\")\n",
        "        os.remove(temp_file)\n",
        "        return result[\"text\"]\n",
        "    except Exception as e:\n",
        "        print(f\"Error in retranscribe_segment: {str(e)}\")\n",
        "        raise\n",
        "    finally:\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "# Очистка памяти перед началом\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(\"Starting transcription...\")\n",
        "\n",
        "if not os.path.exists(audio_file_path):\n",
        "    print(\"Audio file not found. Check the path:\", audio_file_path)\n",
        "    exit(1)\n",
        "\n",
        "print(f\"Loading Whisper model: {selected_model}...\")\n",
        "model = None  # Инициализируем model как None\n",
        "try:\n",
        "    model = whisper.load_model(selected_model)  # Загружаем модель один раз\n",
        "except RuntimeError as e:\n",
        "    print(f\"Failed to load model: {e}\")\n",
        "    print(\"Try using a smaller model (e.g., 'medium' or 'small') or restarting the runtime (Runtime → Restart runtime).\")\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    exit(1)\n",
        "\n",
        "combined_text = \"\"\n",
        "combined_segments = []\n",
        "\n",
        "try:\n",
        "    print(\"Transcribing the entire file...\")\n",
        "    result = model.transcribe(audio_file_path, language=selected_language, verbose=show_text)\n",
        "    combined_text = result[\"text\"]\n",
        "    combined_segments = result.get(\"segments\", [])\n",
        "    for seg in combined_segments:\n",
        "        seg[\"speaker\"] = \"Speaker\"\n",
        "except Exception as e:\n",
        "    print(f\"Error during initial transcription: {e}\")\n",
        "    if 'model' in globals():  # Проверяем, определена ли model\n",
        "        del model\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    exit(1)\n",
        "\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "def create_subtitle_editor(whisper_model):\n",
        "    output = widgets.Output()\n",
        "\n",
        "    def update_srt_file():\n",
        "        with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            for i, seg in enumerate(combined_segments, start=1):\n",
        "                start_time = seconds_to_srt_time(seg[\"start\"])\n",
        "                end_time = seconds_to_srt_time(seg[\"end\"])\n",
        "                speaker = seg.get(\"speaker\", \"Unknown\")\n",
        "                text = seg[\"text\"].strip()\n",
        "                f.write(f\"{i}\\n\")\n",
        "                f.write(f\"{start_time} --> {end_time}\\n\")\n",
        "                f.write(f\"[{speaker}] {text}\\n\\n\")\n",
        "\n",
        "    def on_play_clicked(b, seg_idx):\n",
        "        with output:\n",
        "            output.clear_output()\n",
        "            audio_seg = extract_audio_segment(audio_file_path,\n",
        "                                           combined_segments[seg_idx][\"start\"],\n",
        "                                           combined_segments[seg_idx][\"end\"])\n",
        "            audio_data = audio_seg.export(format=\"wav\").read()\n",
        "            del audio_seg\n",
        "            display(Audio(audio_data, autoplay=True))\n",
        "\n",
        "    def retranscribe_segment_wrapper(seg_idx, progress_label, whisper_model):\n",
        "        print(f\"Starting retranscribe for segment {seg_idx}\")\n",
        "        try:\n",
        "            new_text = retranscribe_segment(audio_path=audio_file_path,\n",
        "                                          start=combined_segments[seg_idx][\"start\"],\n",
        "                                          end=combined_segments[seg_idx][\"end\"],\n",
        "                                          whisper_model=whisper_model,\n",
        "                                          lang=selected_language,\n",
        "                                          progress_label=progress_label)\n",
        "            combined_segments[seg_idx][\"text\"] = new_text\n",
        "            text_boxes[seg_idx].value = new_text\n",
        "            update_srt_file()\n",
        "            progress_label.value = \"Done\"\n",
        "            print(f\"Retranscribe completed for segment {seg_idx}\")\n",
        "        except Exception as e:\n",
        "            progress_label.value = f\"Error: {str(e)}\"\n",
        "            print(f\"Retranscribe failed for segment {seg_idx}: {str(e)}\")\n",
        "\n",
        "    def on_retranscribe_clicked(b, seg_idx, progress_label):\n",
        "        with output:\n",
        "            output.clear_output()\n",
        "            progress_label.value = \"Starting...\"\n",
        "            print(\"Button clicked, launching retranscribe...\")\n",
        "            retranscribe_segment_wrapper(seg_idx, progress_label, whisper_model)\n",
        "\n",
        "    def on_text_changed(change, seg_idx):\n",
        "        combined_segments[seg_idx][\"text\"] = change.new\n",
        "        update_srt_file()\n",
        "\n",
        "    text_boxes = []\n",
        "    for i, seg in enumerate(combined_segments):\n",
        "        start_time = seconds_to_srt_time(seg[\"start\"])\n",
        "        end_time = seconds_to_srt_time(seg[\"end\"])\n",
        "        time_label = widgets.Label(value=f'{start_time} --> {end_time}',\n",
        "                                 layout={'width': '250px'})\n",
        "\n",
        "        text_box = widgets.Textarea(\n",
        "            value=seg[\"text\"],\n",
        "            layout={'width': '500px'}\n",
        "        )\n",
        "        text_box.observe(lambda change, idx=i: on_text_changed(change, idx), names='value')\n",
        "\n",
        "        play_button = widgets.Button(description=\"Play\")\n",
        "        play_button.on_click(lambda b, idx=i: on_play_clicked(b, idx))\n",
        "\n",
        "        retranscribe_button = widgets.Button(description=\"Retranscribe\")\n",
        "        progress_label = widgets.Label(value=\"\", layout={'width': '150px'})\n",
        "\n",
        "        retranscribe_button.on_click(lambda b, idx=i, pl=progress_label: on_retranscribe_clicked(b, idx, pl))\n",
        "\n",
        "        hbox = widgets.HBox([time_label, text_box, play_button, retranscribe_button, progress_label])\n",
        "        text_boxes.append(text_box)\n",
        "        display(hbox)\n",
        "\n",
        "    display(output)\n",
        "\n",
        "if selected_format == \"txt\":\n",
        "    output_file_path = f\"transcription_{timestamp}.txt\"\n",
        "    with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for seg in combined_segments:\n",
        "            speaker = seg.get(\"speaker\", \"Unknown\")\n",
        "            text = seg[\"text\"].strip()\n",
        "            f.write(f\"[{speaker}] {text}\\n\")\n",
        "    print(f\"Transcription completed. Result in file {output_file_path}\")\n",
        "\n",
        "elif selected_format == \"srt\":\n",
        "    output_file_path = f\"transcription_{timestamp}.srt\"\n",
        "    with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for i, seg in enumerate(combined_segments, start=1):\n",
        "            start_time = seconds_to_srt_time(seg[\"start\"])\n",
        "            end_time = seconds_to_srt_time(seg[\"end\"])\n",
        "            speaker = seg.get(\"speaker\", \"Unknown\")\n",
        "            text = seg[\"text\"].strip()\n",
        "            f.write(f\"{i}\\n\")\n",
        "            f.write(f\"{start_time} --> {end_time}\\n\")\n",
        "            f.write(f\"[{speaker}] {text}\\n\\n\")\n",
        "    print(f\"Transcription completed. Result in file {output_file_path}\")\n",
        "\n",
        "    print(\"\\nInteractive Subtitle Editor:\")\n",
        "    create_subtitle_editor(model)\n",
        "\n",
        "else:\n",
        "    print(\"Unknown format. Check the selected format settings.\")\n",
        "\n",
        "# Очистка памяти в конце\n",
        "print(\"Cleaning up memory at the end...\")\n",
        "if 'model' in globals():  # Проверяем, существует ли model\n",
        "    del model\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "r6u7HhzpX3s_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##**Download** { display-mode: \"form\" }\n",
        "\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "if \"output_file_path\" in globals() and output_file_path and os.path.exists(output_file_path):\n",
        "    files.download(output_file_path)\n",
        "    print(f\"File {output_file_path} download to your computer has started.\")\n",
        "else:\n",
        "    print(\"Result file not found. Please complete Step 3 first.\")\n"
      ],
      "metadata": {
        "id": "rUyrzg9aZlA0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}